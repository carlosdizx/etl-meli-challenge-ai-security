# ETL para Procesamiento de Datos con IA

Este proyecto implementa un proceso ETL (Extract, Transform, Load) dise√±ado para obtener 
informaci√≥n de fuentes externas, procesarla utilizando limpieza de datos, y almacenar 
los resultados en un bucket de S3 con cach√© en CDN para un acceso r√°pido.

## üöÄ Descripci√≥n del Proyecto

El ETL se encarga de:
1. **Extraer** datos de fuentes externas
2. **Transformar** los datos para solo utilizar los que se necesitan y limpiar valores nulos
3. **Cargar** los resultados procesados en un bucket de S3
4. Gestionar la cach√© en CDN para un acceso r√°pido a los datos procesados

## üõ† Configuraci√≥n del Entorno

### 1) üì¶ Crear el entorno virtual

Un entorno virtual a√≠sla las dependencias de tu proyecto, evitando conflictos con otras instalaciones de Python.

```bash
python -m venv .venv
```

### 2) ‚ñ∂Ô∏è Activar el entorno virtual

- **Windows (PowerShell)**:
  ```powershell
  .\.venv\Scripts\Activate.ps1
  ```
- **Windows (CMD)**:
  ```cmd
  .venv\Scripts\activate.bat
  ```
- **macOS/Linux (Bash/Zsh)**:
  ```bash
  source .venv/bin/activate
  ```
- **macOS/Linux (Fish)**:
  ```fish
  source .venv/bin/activate.fish
  ```

### 3) üêç Verificar la versi√≥n de Python

```bash
# Windows
.\.venv\Scripts\python.exe --version

# macOS/Linux
.venv/bin/python --version
```

### 4) ‚öôÔ∏è Actualizar pip

```bash
python -m pip install --upgrade pip
```

### 5) ‚ú® Instalar dependencias

```bash
pip install -r requirements.txt
```

### 6) üîë Configurar variables de entorno

Crea el archivo de configuraci√≥n con el siguiente comando:

```bash
python -m scripts.setup_secrets
```

Este script te guiar√° en la configuraci√≥n de los secretos necesarios, como los recursos de AWS para S3 y CloudFront.

### 7) üö¶ Ejecuci√≥n del ETL

### Descarga y procesamiento de datos

```bash
python -m scripts.load_dataset
```

### Entrenar modelo de ML

```bash
python -m scripts.train_models
```

### Subida a S3

```bash
python -m scripts.upload_s3
```

### Invalidar cach√© del CDN (si es necesario)

```bash
python -m scripts.invalidate_cache
```

---

## üìÅ Estructura del Proyecto

- `scripts/`: Contiene todos los scripts de Python para el proceso ETL
  - `load_dataset.py`: Descarga y prepara los datos de origen
  - `train_models.py`: Entrena los modelos de IA
  - `upload_s3.py`: Maneja la carga de archivos a S3
  - `invalidate_cache.py`: Gestiona la invalidaci√≥n de la cach√© del CDN
  - `setup_secrets.py`: Configura las variables de entorno necesarias
- `dto/`: Contiene los modelos de datos utilizados en el proyecto
- `utils/`: Utilidades y helpers para el proyecto

## üîÑ Flujo de Trabajo

1. Los datos son extra√≠dos de kaggle y se cachean en tu equipo para reutilizarlos en futuras ejecuciones
2. Se aplican transformaciones utilizando limpieza profunda y usando solo propiedades requeridas
3. Los resultados se almacenan localmente para validaci√≥n
4. Los archivos procesados se suben a S3
5. Se configura la cach√© del CDN para un acceso r√°pido

## üìù Notas Adicionales

- Aseg√∫rate de tener configuradas las credenciales de AWS con los permisos necesarios para S3 y CloudFront
- El proceso de invalidaci√≥n de cach√© puede tardar hasta 15 minutos en propagarse completamente
